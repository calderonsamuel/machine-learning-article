# Librerias
```{python}
# Importaciones
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
# Librerias probadas Segunda seleccion de variables
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
# Multiple core
from multiprocessing import Pool
```
# Cargar data
```{python}
# Cargar data
df = pd.read_csv("data/OnlineNewsPopularity.csv")
```
# Metodos practicos
```{python}
# Funciones usables reusables
# Eliminar espacios vacios de variables
def borrar_espacios(text_param: str) -> str:
  return text_param.replace(' ', '')
def quitar_espacios_vacíos_df(df_param):
  for key in df.columns:
    df_param.rename(columns={str(key): borrar_espacios(str(key))}, inplace=True)
  return df_param
# Mostrar grafico de correlacion
def show_correlation(df_param):
    # Matriz de correlacion
    matriz_correlacion = df_param.corr()

    plt.matshow(matriz_correlacion, cmap="coolwarm")
    plt.colorbar()
    plt.show()
# Mostrar grafico codo
def show_codo_graf(ve_param, vc_param):
    # Visualizar gráfico de codo
    plt.plot(range(1, len(ve_param) + 1), vc_param)
    plt.xlabel('Número de caracteristicas')
    plt.ylabel('Varianza explicada acumulada')
    plt.grid(True)
    plt.show()
# Forward Selection
def forward_selection(data, target):
    initial_features = data.columns.tolist()
    best_features = []
    while initial_features:
        remaining_features = list(set(initial_features) - set(best_features))
        new_feature = None
        best_score = 0
        for feature in remaining_features:
            current_features = best_features + [feature]
            X_train, X_test, y_train, y_test = train_test_split(data[current_features], target, test_size=0.3, random_state=42)
            model = LogisticRegression(max_iter=10000, multi_class='multinomial', solver='lbfgs', n_jobs=16)
            model.fit(X_train, y_train)
            predictions = model.predict(X_test)
            score = accuracy_score(y_test, predictions)
            if score > best_score:
                best_score = score
                new_feature = feature
        if new_feature:
            best_features.append(new_feature)
        else:
            break
    return best_features
# Ejecucucion paralela Forward Selection
def evaluate_feature(args):
    feature, best_features, data, target = args
    current_features = best_features + [feature]
    X_train, X_test, y_train, y_test = train_test_split(data[current_features], target, test_size=0.3, random_state=42)
    model = LogisticRegression(max_iter=10000)  
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    score = accuracy_score(y_test, predictions)
    return score, feature

def parallel_forward_selection(data, target, n_cores):
    initial_features = data.columns.tolist()
    best_features = []
    
    with Pool(n_cores) as pool:
        while initial_features:
            remaining_features = list(set(initial_features) - set(best_features))
            args = [(feature, best_features, data, target) for feature in remaining_features]
            scores_features = pool.map(evaluate_feature, args)
            best_score, new_feature = max(scores_features)
            if new_feature:
                best_features.append(new_feature)
            else:
                break

    return best_features
```
# Ordenamiento
```{python}
# Mostrar data
df.head(5)
```
# Eliminacion de caracteristicas Irrelevantes
```{python}
# Eliminar variables no ocupadas
df = quitar_espacios_vacíos_df(df)
# Descartando url
df.drop(['url'], axis=1, inplace=True) # Todos los urls estan agregadas pero ya no existen, por esta razon quitamos el campo.
# Descartamos time_delta
df
```
# Matriz de correlacion
```{python}
show_correlation(df)
```
# Normalizar Datos
```{python}
# Normalizar datos
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df)

# pd.DataFrame(df_scaled, columns=df.columns)
```
# Disminuir dimensionalidad con PCA
```{python}
# Crear objeto PCA
pca = PCA()
# Ajustar el modelo PCA a los datos normalizados
pca.fit(df_scaled)
```
# Grafica Codo
```{python}
# Obtener varianza explicada por cada caracteristica
variance_explained = pca.explained_variance_ratio_
# Calcular varianza acumulada
variance_cumulada = np.cumsum(variance_explained)

show_codo_graf(variance_explained, variance_cumulada)
```
# Establecer umbral de varianza explicada
```{python}
# Establecer umbral de varianza explicada
umbral_varianza = 0.8
# Seleccionar caracteristicas principales que cumplan con el umbral
caracteristicas_seleccionados = np.where(variance_cumulada >= umbral_varianza)[0]
# Número de caracteristicas seleccionados
numero_caracteristicas = len(caracteristicas_seleccionados)

print(f"Número de caracteristicas seleccionados para explicar {umbral_varianza:.2f} de la varianza: {numero_caracteristicas}")
```
# Descartar caracteristicas no seleccionadas
```{python}
# Descartar caracteristicas no seleccionados
caracteristicas_descartados = np.where(variance_cumulada < umbral_varianza)[0]
# Obtener nombres de las columnas descartadas
columnas_descartadas = df.columns[caracteristicas_descartados]
# Eliminar columnas descartadas por nombre
reduced_pca_df = df.drop(columnas_descartadas, axis=1)
```
# Forward Selection
```{python}
data_target = reduced_pca_df['shares']
data_features = reduced_pca_df.drop(columns=['shares'])
# selected_features = forward_selection(data_features, data_target)
selected_features = parallel_forward_selection(data_features, data_target, n_cores=16)
print("Selected features:", selected_features)
```
# Imprimir caracteristicas selecionadas y descartados
```{python}
# Imprimir caracteristicas
print('Caracteristicas seleccionadas:')
print(df.columns[caracteristicas_seleccionados])
print('Caracteristicas descartadas:')
print(columnas_descartadas)
```
# Descargar df preparado
```{python}
# Descargar caracteristicas preparadas
reduced_pca_df.to_csv('data/ReadyToTrain.csv', index=False)
```
```{python}
reduced_pca_df
```